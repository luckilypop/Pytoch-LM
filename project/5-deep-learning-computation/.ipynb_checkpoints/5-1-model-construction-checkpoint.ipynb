{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b19e504",
   "metadata": {},
   "source": [
    "# 5.1 层和块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d7bafc",
   "metadata": {},
   "source": [
    "### torch.rand(2,20)\n",
    "生成（2，20）shape的矩阵，其值是0-1均匀分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0cd68a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4751, 0.9540, 0.1680, 0.8983, 0.4683, 0.7930, 0.5388, 0.1430, 0.3689,\n",
       "          0.2020, 0.0114, 0.0428, 0.7735, 0.6363, 0.3890, 0.0556, 0.7098, 0.1910,\n",
       "          0.0902, 0.4743],\n",
       "         [0.4904, 0.1747, 0.5902, 0.5531, 0.5881, 0.8151, 0.8605, 0.1827, 0.7679,\n",
       "          0.8432, 0.0482, 0.2754, 0.3051, 0.5150, 0.5325, 0.4051, 0.4888, 0.0398,\n",
       "          0.0687, 0.6099]]),\n",
       " tensor([[-0.0523, -0.3232, -0.0286,  0.0570,  0.0200,  0.0031, -0.2713, -0.1211,\n",
       "           0.1911,  0.0672],\n",
       "         [ 0.0502, -0.3193,  0.0773, -0.0072,  0.0381, -0.0351, -0.2542, -0.0386,\n",
       "           0.1527,  0.0266]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 举例\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "X,net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66295cf6",
   "metadata": {},
   "source": [
    "## 5.1.1 自定义块\n",
    "我们简要总结一下每个块必须提供的基本功能：\n",
    "\n",
    "1. 将输入数据作为其正向传播函数的参数。\n",
    "1. 通过正向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收任意维的输入，但是返回一个维度256的输出。\n",
    "1. 计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。\n",
    "1. 存储和访问正向传播计算所需的参数。\n",
    "1. 根据需要初始化模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0820b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用`MLP`的父类`Block`的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数`params`（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "\n",
    "    # 定义模型的正向传播，即如何根据输入`X`返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39352cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1626, -0.1957,  0.1502, -0.2620, -0.0560, -0.1671,  0.0596,  0.1572,\n",
       "         -0.0066, -0.1545],\n",
       "        [ 0.1169, -0.0594,  0.0999, -0.1891, -0.0360, -0.0475,  0.0487,  0.1608,\n",
       "         -0.0204, -0.2285]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe269c28",
   "metadata": {},
   "source": [
    "## 5.1.2 顺序块\n",
    "`Sequential`的设计是为了把其他模块串起来。为了构建我们自己的简化的`MySequential`，我们只需要定义两个关键函数：\n",
    "1. 一种将块逐个追加到列表中的函数。\n",
    "2. 一种正向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。\n",
    "\n",
    "下面的`MySequential`类提供了与默认`Sequential`类相同的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b31b4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            # 这里，`block`是`Module`子类的一个实例。我们把它保存在'Module'类的成员变量\n",
    "            # `_modules` 中。`block`的类型是OrderedDict。\n",
    "            self._modules[block] = block\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69e3d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2120, -0.1560,  0.0535,  0.0278, -0.0626,  0.0856, -0.0032, -0.0661,\n",
       "         -0.1709,  0.2355],\n",
       "        [-0.1921, -0.1014, -0.0180, -0.0643, -0.1112,  0.1995,  0.1197,  0.0763,\n",
       "         -0.0741,  0.2440]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0827e45",
   "metadata": {},
   "source": [
    "## 5.1.3 在正向传播函数中执行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "977aae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变。\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及`relu`和`dot`函数。\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数。\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0b8bf40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3552, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b071539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌套块\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1bbced7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0202, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346bc1e5",
   "metadata": {},
   "source": [
    "## 5.1.4 小结\n",
    "* 层也是块。\n",
    "* 一个块可以由许多层组成。\n",
    "* 一个块可以由许多块组成。\n",
    "* 块可以包含代码。\n",
    "* 块负责大量的内部处理，包括参数初始化和反向传播。\n",
    "* 层和块的顺序连接由`Sequential`块处理。"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:d2l] *",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
