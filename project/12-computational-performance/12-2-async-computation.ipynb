{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841966ec",
   "metadata": {},
   "source": [
    "# 12.2. 异步计算\n",
    "解异步编程是如何工作的，通过主动地减少计算需求和相互依赖，有助于我们开发更高效的程序。这使我们能够减少内存开销并提高处理器利用率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e831bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import numpy\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd2806",
   "metadata": {},
   "source": [
    "## 12.2.1. 通过后端异步处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d3f9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "numpy: 0.8839 sec\n",
      "torch: 0.0009 sec\n"
     ]
    }
   ],
   "source": [
    "# 考虑一个简单问题：我们要生成一个随机矩阵并将其相乘。\n",
    "# 让我们在 NumPy 和 PyTorch 张量中都这样做，看看它们的区别。\n",
    "# 请注意，PyTorch 的 tensor 是在 GPU 上定义的。 \n",
    "# GPU 计算热身\n",
    "device = d2l.try_gpu()\n",
    "print(device)\n",
    "a = torch.randn(size=(1000, 1000), device=device)\n",
    "b = torch.mm(a, a)\n",
    "\n",
    "with d2l.Benchmark('numpy'):\n",
    "    for _ in range(10):\n",
    "        a = numpy.random.normal(size=(1000, 1000))\n",
    "        b = numpy.dot(a, a)\n",
    "\n",
    "with d2l.Benchmark('torch'):\n",
    "    for _ in range(10):\n",
    "        a = torch.randn(size=(1000, 1000), device=device)\n",
    "        b = torch.mm(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a88262",
   "metadata": {},
   "source": [
    "- NumPy 点积是在 CPU 上执行的，而 PyTorch 矩阵乘法是在 GPU 上执行的，后者的速度要快得多。\n",
    "- 但巨大的时间差距表明一定还有其他原因。\n",
    "- 默认情况下，GPU 操作在 PyTorch 中是异步的。\n",
    "- 强制 PyTorch 在返回之前完成所有计算，这种强制说明了之前发生的情况：计算是由后端执行，而前端将控制权返回给了 Python。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c71dd305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 0.0199 sec\n"
     ]
    }
   ],
   "source": [
    "with d2l.Benchmark():\n",
    "    for _ in range(10):\n",
    "        a = torch.randn(size=(1000, 1000), device=device)\n",
    "        b = torch.mm(a, a)\n",
    "    torch.cuda.synchronize(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b3990a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "x = torch.ones((1, 2), device=device)\n",
    "y = torch.ones((1, 2), device=device)\n",
    "z = x * y + 2\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdaa990",
   "metadata": {},
   "source": [
    "![后端跟踪计算图中各个步骤之间的依赖关系。](https://zh-v2.d2l.ai/_images/asyncgraph.svg)\n",
    ":label:`fig_asyncgraph`\n",
    "\n",
    "上面的代码片段在 :numref:`fig_asyncgraph` 中进行了说明。每当 Python 前端线程执行前三条语句中的一条语句时，它只是将任务返回到后端队列。当最后一个语句的结果需要被打印出来时，Python前端线程将等待 C++ 后端线程完成变量 `z` 的结果计算。这种设计的一个好处是 Python 前端线程不需要执行实际的计算。因此，不管 Python 的性能如何，对程序的整体性能几乎没有影响。 :numref:`fig_threading` 演示了前端和后端如何交互。\n",
    "\n",
    "![前端和后端的交互。](https://zh-v2.d2l.ai/_images/threading.svg)\n",
    ":label:`fig_threading`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc530d",
   "metadata": {},
   "source": [
    "## 12.2.2 障碍器与阻塞器\n",
    "## 12.2.3 改进计算\n",
    "## 12.2.4 小结\n",
    "\n",
    "* 深度学习框架可以将 Python 前端的控制与后端的执行解耦，使得命令可以快速地异步插入后端、并行执行。\n",
    "* 异步产生了一个相当灵活的前端，但请注意：过度填充任务队列可能会导致内存消耗过多。建议对每个小批量进行同步，以保持前端和后端大致同步。\n",
    "* 芯片供应商提供了复杂的性能分析工具，以获得对深度学习效率更精确的洞察。"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:d2l] *",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
