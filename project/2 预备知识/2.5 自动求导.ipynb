{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccf386bc",
   "metadata": {},
   "source": [
    "# 2.5 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fc91ec",
   "metadata": {},
   "source": [
    "## 2.5.1 一个简单例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82b67f",
   "metadata": {},
   "source": [
    "假设我们想对函数y=2x⊤x关于列向量x求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4445ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19e132a",
   "metadata": {},
   "source": [
    "### requires_grad() 反向传播\n",
    "是Pytorch中通用数据结构Tensor的一个属性，用于说明当前量是否需要在计算中保留对应的梯度信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52aa3108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x.requires_grad_(True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a371ae0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba419c",
   "metadata": {},
   "source": [
    "反向传播函数来自动计算y关于x每个分量的梯度，并打印这些梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5373ec5",
   "metadata": {},
   "source": [
    "### backward()\n",
    "创建一个张量x，并设置其 requires_grad参数为True，程序将会追踪所有对于该张量的操作，当完成计算后通过调用 .backward()，自动计算所有的梯度， 这个张量的所有梯度将会自动积累到 .grad 属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec2bac07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f9aff5",
   "metadata": {},
   "source": [
    "函数y=2x⊤x关于x的梯度应为4x。让我们快速验证我们想要的梯度是否正确计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6fd58cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2044d3b3",
   "metadata": {},
   "source": [
    "现在让我们计算x的另一个函数。y = x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c5cf991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before_x_grad=tensor([ 0.,  4.,  8., 12.])\n",
      "after0_x_grad=tensor([0., 0., 0., 0.]), x=tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "y=6.0\n",
      "after1_x_grad=tensor([1., 1., 1., 1.]), x=tensor([0., 1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(f'before_x_grad={x.grad}')\n",
    "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "x.grad.zero_()\n",
    "print(f'after0_x_grad={x.grad}, x={x}')\n",
    "y = x.sum()\n",
    "print(f'y={y}')\n",
    "y.backward()\n",
    "print(f'after1_x_grad={x.grad}, x={x}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b47451",
   "metadata": {},
   "source": [
    "## 2.5.2 非标量变量的反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389dd76c",
   "metadata": {},
   "source": [
    "对非标量调用`backward`需要传入一个`gradient`参数，该参数指定微分函数关于`self`的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee57f96",
   "metadata": {},
   "source": [
    "# ?、?torch.ones(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf37726",
   "metadata": {},
   "source": [
    "### y.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f7e3e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>),y_sum=14.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在我们的例子中，我们只想求偏导数的和，所以传递一个1的梯度是合适的\n",
    "x.grad.zero_()\n",
    "y = x * x #两个矩阵的按元素乘法称为Hadamard积\n",
    "print(f'y={y},y_sum={y.sum()}')\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5635fe66",
   "metadata": {},
   "source": [
    "## 2.5.3 分离计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132d0577",
   "metadata": {},
   "source": [
    "下面的反向传播函数计算z=u*x关于x的偏导数，同时将u作为常数处理，而不是z=x*x*x关于x的偏导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e0d63d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>),\n",
       " tensor([0., 1., 4., 9.]),\n",
       " tensor([ 0.,  1.,  8., 27.], grad_fn=<MulBackward0>),\n",
       " tensor([0., 1., 2., 3.], requires_grad=True),\n",
       " tensor([0., 1., 4., 9.]),\n",
       " tensor([True, True, True, True]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "\n",
    "y,u,z,x,x.grad,x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd99d196",
   "metadata": {},
   "source": [
    "记录了y的计算结果，我们可以随后在y上调用反向传播，得到y=x*x关于的x的导数，这里是2*%xmode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fba92250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a157b1",
   "metadata": {},
   "source": [
    "## 2.5.4 Python控制流的梯度计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c19a0",
   "metadata": {},
   "source": [
    "### grad_fn： grad_fn用来记录变量是怎么来的\n",
    "### nor() 1范数，绝对值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d08d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    print(a)\n",
    "    b = a * 2\n",
    "    print(b)\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "        print(b)\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8874671e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5637, requires_grad=True)\n",
      "tensor(-1.1274, grad_fn=<MulBackward0>)\n",
      "tensor(-2.2549, grad_fn=<MulBackward0>)\n",
      "tensor(-4.5098, grad_fn=<MulBackward0>)\n",
      "tensor(-9.0195, grad_fn=<MulBackward0>)\n",
      "tensor(-18.0391, grad_fn=<MulBackward0>)\n",
      "tensor(-36.0782, grad_fn=<MulBackward0>)\n",
      "tensor(-72.1563, grad_fn=<MulBackward0>)\n",
      "tensor(-144.3126, grad_fn=<MulBackward0>)\n",
      "tensor(-288.6252, grad_fn=<MulBackward0>)\n",
      "tensor(-577.2505, grad_fn=<MulBackward0>)\n",
      "tensor(-1154.5010, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2236cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad 就是求b = 2*……2 * x <1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9aa608e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(204800.))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d / a, a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0799ef1d",
   "metadata": {},
   "source": [
    "## 2.5.5 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f6297",
   "metadata": {},
   "source": [
    "深度学习框架可以自动计算导数。\n",
    "\n",
    "为了使用它，我们首先将梯度附加到想要对其计算偏导数的变量上。\n",
    "\n",
    "然后我们记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af8adc",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## 2.5.6 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a43c9cb",
   "metadata": {},
   "source": [
    "1.为什么计算二阶导数比一阶导数的开销要更大？\n",
    "\n",
    "2.在运行反向传播函数之后，立即再次运行它，看看会发生什么。\n",
    "\n",
    "3.在控制流的例子中，我们计算d关于a的导数，如果我们将变量a更改为随机向量或矩阵，会发生什么？此时，计算结果f(a)不再是标量。结果会发生什么？我们如何分析这个结果？\n",
    "\n",
    "4.重新设计一个求控制流梯度的例子。运行并分析结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l] *",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
