{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9aa695",
   "metadata": {},
   "source": [
    "# 2.3 线性代数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694adfc4",
   "metadata": {},
   "source": [
    "## 2.3.1 标量\n",
    "标量（scalar）仅包含一个数值，由只有一个元素的张量表示\n",
    "\n",
    "变量（variable），它们表示未知的标量值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "671c37c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.]), tensor([6.]), tensor([1.5000]), tensor([9.]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([3.0]) \n",
    "y = torch.tensor([2.0])\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d3f9a",
   "metadata": {},
   "source": [
    "## 2.3.2 向量\n",
    "将向量视为标量值组成的列表。我们将这些标量值称为向量的元素（element）或分量（component）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d78a0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor(3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4) \n",
    "x, x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e20d650",
   "metadata": {},
   "source": [
    "## 2.3.3 长度、维度和形状"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f1f89",
   "metadata": {},
   "source": [
    "### len()访问张量的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a58c91ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535841c3",
   "metadata": {},
   "source": [
    "### .shape属性访问向量的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52ed36dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce82e2d",
   "metadata": {},
   "source": [
    "## 2.3.4 矩阵\n",
    "矩阵是向量的推广"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a166b4",
   "metadata": {},
   "source": [
    "### 实例化矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31ab875d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#实例化矩阵\n",
    "A = torch.arange(20).reshape(5, 4)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e31af2",
   "metadata": {},
   "source": [
    "### 矩阵装置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d4fab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8, 12, 16],\n",
       "        [ 1,  5,  9, 13, 17],\n",
       "        [ 2,  6, 10, 14, 18],\n",
       "        [ 3,  7, 11, 15, 19]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#矩阵装置 \n",
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2615963",
   "metadata": {},
   "source": [
    "### 矩阵比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d739324e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#矩阵比较 \n",
    "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]]) \n",
    "B == B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f7e58",
   "metadata": {},
   "source": [
    "## 2.3.5 张量\n",
    "张量为我们提供了描述具有任意数量轴的n维数组的通用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3e43fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62163483",
   "metadata": {},
   "source": [
    "## 2.3.6 张量算法的基本性质"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d1c84",
   "metadata": {},
   "source": [
    "### 矩阵之间加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46ddcb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4) \n",
    "B = A.clone() # 通过分配新内存，将A的一个副本分配给B \n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c63f76",
   "metadata": {},
   "source": [
    "### 矩阵和标量之间加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac7e07cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2 \n",
    "X = torch.arange(24).reshape(2, 3, 4) \n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfef149",
   "metadata": {},
   "source": [
    "### Hadamard积\n",
    "两个矩阵的按元素乘法称为*Hadamard积*（Hadamard product）（数学符号$\\odot$）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "907fd9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fab1a",
   "metadata": {},
   "source": [
    "### mean() 求平均值\n",
    "### numle() 求总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b29097ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, tensor(190.), tensor(9.5000), tensor(9.5000))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.numel(),A.sum(),A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d649c90a",
   "metadata": {},
   "source": [
    "## 2.3.6 降维"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3ea63",
   "metadata": {},
   "source": [
    "### sum()求和函数\n",
    "调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "976c2558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float32) \n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48d92cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), tensor(190.))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5818bec2",
   "metadata": {},
   "source": [
    "### 指定张量沿哪一个轴来通过求和降低维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4bceb9",
   "metadata": {},
   "source": [
    "#### 通过行的元素降维axis=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20d4f414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([40., 45., 50., 55.]),\n",
       " torch.Size([4]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过行的元素降维axis=0，所有列元素求和\n",
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A,A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a79407",
   "metadata": {},
   "source": [
    "#### 通过列的元素降维axis=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "510df1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([ 6., 22., 38., 54., 70.]),\n",
       " torch.Size([5]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过列的元素降维axis=1，所有行元素求和\n",
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A,A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e1ab4",
   "metadata": {},
   "source": [
    "#### 沿着行列求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b648fa34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(190.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 沿着行列求和\n",
    "A.sum(axis=[0, 1])  # Same as `A.sum()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffe5c1f",
   "metadata": {},
   "source": [
    "#### 平均值函数降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d66d248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " tensor([40., 45., 50., 55.]),\n",
       " tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([ 8.,  9., 10., 11.]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape[0],A.sum(axis=0),A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4c14e",
   "metadata": {},
   "source": [
    "## 2.3.7 非降维求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c6a0403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.],\n",
       "        [22.],\n",
       "        [38.],\n",
       "        [54.],\n",
       "        [70.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#调用函数来计算总和或均值时保持轴数不变\n",
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28dddf25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
       "         [0.1818, 0.2273, 0.2727, 0.3182],\n",
       "         [0.2105, 0.2368, 0.2632, 0.2895],\n",
       "         [0.2222, 0.2407, 0.2593, 0.2778],\n",
       "         [0.2286, 0.2429, 0.2571, 0.2714]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A,A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09efce71",
   "metadata": {},
   "source": [
    "### cumsum()按行计算总和\n",
    "此函数不会沿任何轴降低输入张量的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b340ee8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  6.,  8., 10.],\n",
       "         [12., 15., 18., 21.],\n",
       "         [24., 28., 32., 36.],\n",
       "         [40., 45., 50., 55.]]),\n",
       " tensor([[ 0.,  1.,  3.,  6.],\n",
       "         [ 4.,  9., 15., 22.],\n",
       "         [ 8., 17., 27., 38.],\n",
       "         [12., 25., 39., 54.],\n",
       "         [16., 33., 51., 70.]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A,A.cumsum(axis=0),A.cumsum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e4fe4",
   "metadata": {},
   "source": [
    "## 2.3.8 点积\n",
    "点积（dotproduct）是相同位置的按元素乘积的和"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062342ba",
   "metadata": {},
   "source": [
    "### dot()执行点积操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66d3e2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(4, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708b603",
   "metadata": {},
   "source": [
    "### 按元素乘法，然后进行求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa920bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898ecf96",
   "metadata": {},
   "source": [
    "## 2.3.9 矩阵-向量积\n",
    "矩阵向量积$\\mathbf{A}\\mathbf{x}$是一个长度为$m$的列向量，其第$i$个元素是点积$\\mathbf{a}^\\top_i \\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e343c",
   "metadata": {},
   "source": [
    "### np.dot 求解矩阵向量积\n",
    "A的列维数（沿轴1的长度）必须与x的维数（其长度）相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76d36969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14.,  38.,  62.,  86., 110.], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.dot(A,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d600e6f",
   "metadata": {},
   "source": [
    "### np.dot 求解矩阵向量积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "264cce12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([0., 1., 2., 3.]),\n",
       " torch.Size([5, 4]),\n",
       " torch.Size([4]),\n",
       " tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A,x,A.shape, x.shape, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba20e6e",
   "metadata": {},
   "source": [
    "## 2.3.10 矩阵-矩阵乘法\n",
    "矩阵-矩阵乘法$\\mathbf{AB}$看作是简单地执行$m$次矩阵-向量积，并将结果拼接在一起，形成一个$n \\times m$矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e26f21",
   "metadata": {},
   "source": [
    "### mm()矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a7a7be82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  6.,  6.],\n",
       "        [22., 22., 22.],\n",
       "        [38., 38., 38.],\n",
       "        [54., 54., 54.],\n",
       "        [70., 70., 70.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d0e37",
   "metadata": {},
   "source": [
    "## 2.3.11 范数\n",
    "定义：目标，或许是深度学习算法最重要的组成部分（除了数据），被表达为范数。\n",
    "\n",
    "$L_2$*范数*是向量元素平方和的平方根：\n",
    "$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}$$\n",
    "\n",
    "$L_1$范数，它表示为向量元素的绝对值之和：\n",
    "$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|$$\n",
    "\n",
    "矩阵\n",
    "的*Frobenius范数*（Frobenius norm）是矩阵元素平方和的平方根：\n",
    "$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193bbd91",
   "metadata": {},
   "source": [
    "### abs.sum()L1范数求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40604541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1886f867",
   "metadata": {},
   "source": [
    "### norm()L2范数求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd7edadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd9661",
   "metadata": {},
   "source": [
    "### Frobenius范数范数求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68bfc5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd977d33",
   "metadata": {},
   "source": [
    "## 2.3.12 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e92c28",
   "metadata": {},
   "source": [
    "* 标量、向量、矩阵和张量是线性代数中的基本数学对象。\n",
    "\n",
    "* 向量泛化自标量，矩阵泛化自向量。\n",
    "\n",
    "* 标量、向量、矩阵和张量分别具有零、一、二和任意数量的轴。\n",
    "\n",
    "* 一个张量可以通过sum和mean沿指定的轴降低维度。\n",
    "\n",
    "* 两个矩阵的按元素乘法被称为他们的哈达玛积。它与矩阵乘法不同。\n",
    "\n",
    "* 在深度学习中，我们经常使用范数，如\\(L_1\\)范数、\\(L_2\\)范数和弗罗贝尼乌斯范数。\n",
    "\n",
    "* 我们可以对标量、向量、矩阵和张量执行各种操作。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd5320e",
   "metadata": {},
   "source": [
    "## 2.3.13 作业"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6191f1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    证明一个矩阵\\(\\mathbf{A}\\)的转置的转置是\\(\\mathbf{A}\\)：\\((\\mathbf{A}^\\top)^\\top = \\mathbf{A}\\)。\n",
    "\n",
    "    给出两个矩阵\\(\\mathbf{A}\\)和\\(\\mathbf{B}\\)，显示转置的和等于和的转置：\\(\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top\\)。\n",
    "\n",
    "    给定任意方矩阵\\(\\mathbf{A}\\)，\\(\\mathbf{A} + \\mathbf{A}^\\top\\)总是对称的吗?为什么? 1.我们在本节中定义了形状（2,3,4）的张量X。len(X)的输出结果是什么？ 1.对于任意形状的张量X,len(X)是否总是对应于X特定轴的长度?这个轴是什么? 1.运行A/A.sum(axis=1)，看看会发生什么。你能分析原因吗？ 1.当你在曼哈顿的两点之间旅行时，你需要在坐标上走多远，也就是说，就大街和街道而言？你能斜着走吗？ 1.考虑一个具有形状（2,3,4）的张量，在轴0,1,2上的求和输出是什么形状? 1.向linalg.norm函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721bc919",
   "metadata": {},
   "source": [
    "#### 作业1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc544d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165.327px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
